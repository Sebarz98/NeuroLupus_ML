{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: boostaroota in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (1.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from boostaroota) (1.23.1)\n",
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from boostaroota) (2.0.1)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from boostaroota) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from pandas->boostaroota) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from pandas->boostaroota) (2023.3)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from xgboost->boostaroota) (1.10.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->boostaroota) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn xgboost\n",
    "!pip install boostaroota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)  # Set the random seed to 42 for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° of patients: 47\n",
      "N° of columns: 932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>NP-SLE</th>\n",
       "      <th>Event</th>\n",
       "      <th>Scale factor</th>\n",
       "      <th>SNR</th>\n",
       "      <th>White Matter (WM) volume cm3</th>\n",
       "      <th>White Matter (WM) volume %</th>\n",
       "      <th>Normal Appearing White Matter volume cm3</th>\n",
       "      <th>...</th>\n",
       "      <th>FO left thickness mm</th>\n",
       "      <th>FO left thickness norm.</th>\n",
       "      <th>FO thickness asymmetry</th>\n",
       "      <th>PO total thickness mm</th>\n",
       "      <th>PO total thickness norm.</th>\n",
       "      <th>PO right thickness mm</th>\n",
       "      <th>PO right thickness norm.</th>\n",
       "      <th>PO left thickness mm</th>\n",
       "      <th>PO left thickness norm.</th>\n",
       "      <th>PO thickness asymmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paziente 1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>Mood abnormalities (depressive)</td>\n",
       "      <td>0.67586</td>\n",
       "      <td>42.3566</td>\n",
       "      <td>438.3091</td>\n",
       "      <td>35.4223</td>\n",
       "      <td>438.2523</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2623</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>18.2292</td>\n",
       "      <td>2.4475</td>\n",
       "      <td>0.022797</td>\n",
       "      <td>2.2930</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>2.5970</td>\n",
       "      <td>0.024190</td>\n",
       "      <td>-12.4336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paziente 2</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>Na</td>\n",
       "      <td>0.70729</td>\n",
       "      <td>105.5166</td>\n",
       "      <td>472.6302</td>\n",
       "      <td>37.2214</td>\n",
       "      <td>466.0998</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8574</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>-18.2462</td>\n",
       "      <td>1.3628</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>1.2929</td>\n",
       "      <td>0.011940</td>\n",
       "      <td>1.4317</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>-10.1909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paziente 3</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>Na</td>\n",
       "      <td>0.65236</td>\n",
       "      <td>49.4839</td>\n",
       "      <td>407.0018</td>\n",
       "      <td>33.7657</td>\n",
       "      <td>406.9770</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6216</td>\n",
       "      <td>0.024634</td>\n",
       "      <td>6.8561</td>\n",
       "      <td>2.3106</td>\n",
       "      <td>0.021711</td>\n",
       "      <td>2.4840</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>2.1159</td>\n",
       "      <td>0.019882</td>\n",
       "      <td>16.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paziente 4</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>0.65564</td>\n",
       "      <td>44.8080</td>\n",
       "      <td>424.9121</td>\n",
       "      <td>35.6460</td>\n",
       "      <td>424.8701</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0341</td>\n",
       "      <td>0.028616</td>\n",
       "      <td>-6.5858</td>\n",
       "      <td>2.1641</td>\n",
       "      <td>0.020410</td>\n",
       "      <td>2.2997</td>\n",
       "      <td>0.021689</td>\n",
       "      <td>2.0193</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>12.9849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paziente 5</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>Na</td>\n",
       "      <td>0.76373</td>\n",
       "      <td>94.5834</td>\n",
       "      <td>548.5729</td>\n",
       "      <td>41.4234</td>\n",
       "      <td>547.8604</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9152</td>\n",
       "      <td>0.035652</td>\n",
       "      <td>-10.4521</td>\n",
       "      <td>2.5960</td>\n",
       "      <td>0.023640</td>\n",
       "      <td>2.5593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>2.6209</td>\n",
       "      <td>0.023866</td>\n",
       "      <td>-2.3788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 932 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Patient  Gender  Age  NP-SLE                            Event  \\\n",
       "0  Paziente 1       0   38       1  Mood abnormalities (depressive)   \n",
       "1  Paziente 2       0   41       0                              Na    \n",
       "2  Paziente 3       0   32       0                              Na    \n",
       "3  Paziente 4       0   31       1                          Seizure   \n",
       "4  Paziente 5       0   43       0                              Na    \n",
       "\n",
       "   Scale factor       SNR  White Matter (WM) volume cm3  \\\n",
       "0       0.67586   42.3566                      438.3091   \n",
       "1       0.70729  105.5166                      472.6302   \n",
       "2       0.65236   49.4839                      407.0018   \n",
       "3       0.65564   44.8080                      424.9121   \n",
       "4       0.76373   94.5834                      548.5729   \n",
       "\n",
       "   White Matter (WM) volume %  Normal Appearing White Matter volume cm3  ...  \\\n",
       "0                     35.4223                                  438.2523  ...   \n",
       "1                     37.2214                                  466.0998  ...   \n",
       "2                     33.7657                                  406.9770  ...   \n",
       "3                     35.6460                                  424.8701  ...   \n",
       "4                     41.4234                                  547.8604  ...   \n",
       "\n",
       "   FO left thickness mm  FO left thickness norm.  FO thickness asymmetry  \\\n",
       "0                2.2623                 0.021072                 18.2292   \n",
       "1                1.8574                 0.017152                -18.2462   \n",
       "2                2.6216                 0.024634                  6.8561   \n",
       "3                3.0341                 0.028616                 -6.5858   \n",
       "4                3.9152                 0.035652                -10.4521   \n",
       "\n",
       "   PO total thickness mm  PO total thickness norm.  PO right thickness mm  \\\n",
       "0                 2.4475                  0.022797                 2.2930   \n",
       "1                 1.3628                  0.012585                 1.2929   \n",
       "2                 2.3106                  0.021711                 2.4840   \n",
       "3                 2.1641                  0.020410                 2.2997   \n",
       "4                 2.5960                  0.023640                 2.5593   \n",
       "\n",
       "   PO right thickness norm.  PO left thickness mm  PO left thickness norm.  \\\n",
       "0                  0.021358                2.5970                 0.024190   \n",
       "1                  0.011940                1.4317                 0.013222   \n",
       "2                  0.023341                2.1159                 0.019882   \n",
       "3                  0.021689                2.0193                 0.019045   \n",
       "4                  0.023305                2.6209                 0.023866   \n",
       "\n",
       "   PO thickness asymmetry  \n",
       "0                -12.4336  \n",
       "1                -10.1909  \n",
       "2                 16.0040  \n",
       "3                 12.9849  \n",
       "4                 -2.3788  \n",
       "\n",
       "[5 rows x 932 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "\n",
    "file_path = \"/Users/Sebastiano/data/ML_MRI copy.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"N° of patients: {}\".format(len(df)))\n",
    "print(\"N° of columns: {}\".format(df.shape[1]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective features to consider: 925 \n"
     ]
    }
   ],
   "source": [
    "# Drop unwanted columns\n",
    "\n",
    "df = df.drop(['Patient', 'Gender', 'Age','Event', 'Scale factor', 'SNR'], axis = 'columns')\n",
    "# drop columns that include \"%\" in their name\n",
    "#cols_to_drop = [col for col in df.columns if \"%\" in col]\n",
    "#df = df.drop(columns=cols_to_drop)\n",
    "print(\"Effective features to consider: {} \".format(len(df.columns)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_to_normalize = df.columns.difference(['NP-SLE'])  # Assuming 'target' is the column to predict\n",
    "\n",
    "# Normalize the selected features\n",
    "scaler = MinMaxScaler()\n",
    "df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         SLE       1.00      0.50      0.67         4\n",
      "       NPSLE       0.50      1.00      0.67         2\n",
      "     Control       1.00      1.00      1.00         4\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.83      0.83      0.78        10\n",
      "weighted avg       0.90      0.80      0.80        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your features and target variable\n",
    "X = df.drop(['NP-SLE'], axis=1)  # Replace 'Target_Column_Name' with the name of your target variable\n",
    "y = df['NP-SLE']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=['SLE', 'NPSLE', 'Control'])\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', num_class=3, missing=1, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     0\n",
      "2     0\n",
      "3     1\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     1\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    0\n",
      "12    0\n",
      "13    1\n",
      "15    0\n",
      "16    0\n",
      "18    1\n",
      "19    1\n",
      "20    0\n",
      "21    1\n",
      "22    1\n",
      "23    1\n",
      "24    1\n",
      "26    0\n",
      "27    2\n",
      "28    2\n",
      "29    2\n",
      "30    2\n",
      "31    2\n",
      "32    2\n",
      "33    2\n",
      "35    2\n",
      "36    2\n",
      "37    2\n",
      "38    2\n",
      "39    2\n",
      "40    2\n",
      "41    2\n",
      "42    2\n",
      "43    2\n",
      "44    2\n",
      "45    2\n",
      "46    2\n",
      "Name: NP-SLE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[17:30:10] /Users/runner/work/xgboost/xgboost/src/data/data.cc:501: Check failed: this->labels.Size() % this->num_row_ == 0 (6 vs. 0) : Incorrect size for labels.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000014a004998 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000014a0c11fc xgboost::MetaInfo::SetInfoFromHost(xgboost::Context const&, xgboost::StringView, xgboost::Json) + 732\n  [bt] (2) 3   libxgboost.dylib                    0x000000014a0c0dc4 xgboost::MetaInfo::SetInfo(xgboost::Context const&, xgboost::StringView, xgboost::StringView) + 164\n  [bt] (3) 4   libxgboost.dylib                    0x000000014a01b68c XGDMatrixSetInfoFromInterface + 224\n  [bt] (4) 5   libffi.8.dylib                      0x0000000100eb004c ffi_call_SYSV + 76\n  [bt] (5) 6   libffi.8.dylib                      0x0000000100ead7d4 ffi_call_int + 1336\n  [bt] (6) 7   _ctypes.cpython-310-darwin.so       0x0000000100e9011c _ctypes_callproc + 944\n  [bt] (7) 8   _ctypes.cpython-310-darwin.so       0x0000000100e8a3f8 PyCFuncPtr_call + 228\n  [bt] (8) 9   python3.10                          0x00000001006ef000 _PyEval_EvalFrameDefault + 58988\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/Sebastiano/SLE_ML/XGBoost.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Initialize and train the XGBoost classifier with the best hyperparameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m xgb_clf \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mxgb_params)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m xgb_clf\u001b[39m.\u001b[39;49mfit(X_train, y_train, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, eval_set\u001b[39m=\u001b[39;49m[(X_train, y_train), (X_test, y_test)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Prepare evaluation metric plots\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m results \u001b[39m=\u001b[39m xgb_clf\u001b[39m.\u001b[39mevals_result()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/sklearn.py:1496\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1485\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mnum_class\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_\n\u001b[1;32m   1487\u001b[0m (\n\u001b[1;32m   1488\u001b[0m     model,\n\u001b[1;32m   1489\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1495\u001b[0m )\n\u001b[0;32m-> 1496\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1497\u001b[0m     missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[1;32m   1498\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m   1499\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   1500\u001b[0m     group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1501\u001b[0m     qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1502\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1503\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1504\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m   1505\u001b[0m     eval_set\u001b[39m=\u001b[39;49meval_set,\n\u001b[1;32m   1506\u001b[0m     sample_weight_eval_set\u001b[39m=\u001b[39;49msample_weight_eval_set,\n\u001b[1;32m   1507\u001b[0m     base_margin_eval_set\u001b[39m=\u001b[39;49mbase_margin_eval_set,\n\u001b[1;32m   1508\u001b[0m     eval_group\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1509\u001b[0m     eval_qid\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1510\u001b[0m     create_dmatrix\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dmatrix,\n\u001b[1;32m   1511\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menable_categorical,\n\u001b[1;32m   1512\u001b[0m     feature_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_types,\n\u001b[1;32m   1513\u001b[0m )\n\u001b[1;32m   1515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1516\u001b[0m     params,\n\u001b[1;32m   1517\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   1527\u001b[0m )\n\u001b[1;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/sklearn.py:534\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    515\u001b[0m     missing: \u001b[39mfloat\u001b[39m,\n\u001b[1;32m    516\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    531\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[39mstr\u001b[39m]]]:\n\u001b[1;32m    532\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m     train_dmatrix \u001b[39m=\u001b[39m create_dmatrix(\n\u001b[1;32m    535\u001b[0m         data\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    536\u001b[0m         label\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    537\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    538\u001b[0m         qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m    539\u001b[0m         weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    540\u001b[0m         base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m    541\u001b[0m         feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m    542\u001b[0m         missing\u001b[39m=\u001b[39;49mmissing,\n\u001b[1;32m    543\u001b[0m         enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m    544\u001b[0m         feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    545\u001b[0m         ref\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    546\u001b[0m     )\n\u001b[1;32m    548\u001b[0m     n_validation \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m eval_set \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(eval_set)\n\u001b[1;32m    550\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/sklearn.py:954\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39mif\u001b[39;00m _can_use_qdm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_method) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbooster \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgblinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    953\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m         \u001b[39mreturn\u001b[39;00m QuantileDMatrix(\n\u001b[1;32m    955\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, ref\u001b[39m=\u001b[39;49mref, nthread\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, max_bin\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_bin\n\u001b[1;32m    956\u001b[0m         )\n\u001b[1;32m    957\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m    958\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:1528\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m   1509\u001b[0m         info \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m         \u001b[39mfor\u001b[39;00m info \u001b[39min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1521\u001b[0m         )\n\u001b[1;32m   1522\u001b[0m     ):\n\u001b[1;32m   1523\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1524\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf data iterator is used as input, data like label should be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1525\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mspecified as batch argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1526\u001b[0m         )\n\u001b[0;32m-> 1528\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(\n\u001b[1;32m   1529\u001b[0m     data,\n\u001b[1;32m   1530\u001b[0m     ref\u001b[39m=\u001b[39;49mref,\n\u001b[1;32m   1531\u001b[0m     label\u001b[39m=\u001b[39;49mlabel,\n\u001b[1;32m   1532\u001b[0m     weight\u001b[39m=\u001b[39;49mweight,\n\u001b[1;32m   1533\u001b[0m     base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[1;32m   1534\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m   1535\u001b[0m     qid\u001b[39m=\u001b[39;49mqid,\n\u001b[1;32m   1536\u001b[0m     label_lower_bound\u001b[39m=\u001b[39;49mlabel_lower_bound,\n\u001b[1;32m   1537\u001b[0m     label_upper_bound\u001b[39m=\u001b[39;49mlabel_upper_bound,\n\u001b[1;32m   1538\u001b[0m     feature_weights\u001b[39m=\u001b[39;49mfeature_weights,\n\u001b[1;32m   1539\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[1;32m   1540\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m   1541\u001b[0m     enable_categorical\u001b[39m=\u001b[39;49menable_categorical,\n\u001b[1;32m   1542\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:1587\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1575\u001b[0m config \u001b[39m=\u001b[39m make_jcargs(\n\u001b[1;32m   1576\u001b[0m     nthread\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnthread, missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing, max_bin\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_bin\n\u001b[1;32m   1577\u001b[0m )\n\u001b[1;32m   1578\u001b[0m ret \u001b[39m=\u001b[39m _LIB\u001b[39m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[1;32m   1579\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1580\u001b[0m     it\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     ctypes\u001b[39m.\u001b[39mbyref(handle),\n\u001b[1;32m   1586\u001b[0m )\n\u001b[0;32m-> 1587\u001b[0m it\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1588\u001b[0m \u001b[39m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m _check_call(ret)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:575\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:556\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m dft_ret\n\u001b[1;32m    555\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m fn()\n\u001b[1;32m    557\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[39m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[39m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[39m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:640\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_ref \u001b[39m=\u001b[39m ref\n\u001b[1;32m    639\u001b[0m \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_exception(\u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext(input_data), \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/data.py:1280\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mit \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1280\u001b[0m input_data(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m   1281\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:632\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_temporary_data \u001b[39m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[1;32m    631\u001b[0m dispatch_proxy_set_data(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproxy, new, cat_codes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_host)\n\u001b[0;32m--> 632\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproxy\u001b[39m.\u001b[39;49mset_info(\n\u001b[1;32m    633\u001b[0m     feature_names\u001b[39m=\u001b[39;49mfeature_names,\n\u001b[1;32m    634\u001b[0m     feature_types\u001b[39m=\u001b[39;49mfeature_types,\n\u001b[1;32m    635\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    636\u001b[0m )\n\u001b[1;32m    637\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_ref \u001b[39m=\u001b[39m ref\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:931\u001b[0m, in \u001b[0;36mDMatrix.set_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[1;32m    930\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_label(label)\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_weight(weight)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:1069\u001b[0m, in \u001b[0;36mDMatrix.set_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Set label of dmatrix\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \n\u001b[1;32m   1062\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[39m    The label information to be set into DMatrix\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_meta_backend\n\u001b[0;32m-> 1069\u001b[0m dispatch_meta_backend(\u001b[39mself\u001b[39;49m, label, \u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfloat\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/data.py:1218\u001b[0m, in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39mif\u001b[39;00m _is_np_array_like(data):\n\u001b[0;32m-> 1218\u001b[0m     _meta_from_numpy(data, name, dtype, handle)\n\u001b[1;32m   1219\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[39mif\u001b[39;00m _is_pandas_df(data):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/data.py:1159\u001b[0m, in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMasked array is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1158\u001b[0m interface_str \u001b[39m=\u001b[39m _array_interface(data)\n\u001b[0;32m-> 1159\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39;49mXGDMatrixSetInfoFromInterface(handle, c_str(field), interface_str))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages/xgboost/core.py:281\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[1;32m    272\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [17:30:10] /Users/runner/work/xgboost/xgboost/src/data/data.cc:501: Check failed: this->labels.Size() % this->num_row_ == 0 (6 vs. 0) : Incorrect size for labels.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x000000014a004998 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x000000014a0c11fc xgboost::MetaInfo::SetInfoFromHost(xgboost::Context const&, xgboost::StringView, xgboost::Json) + 732\n  [bt] (2) 3   libxgboost.dylib                    0x000000014a0c0dc4 xgboost::MetaInfo::SetInfo(xgboost::Context const&, xgboost::StringView, xgboost::StringView) + 164\n  [bt] (3) 4   libxgboost.dylib                    0x000000014a01b68c XGDMatrixSetInfoFromInterface + 224\n  [bt] (4) 5   libffi.8.dylib                      0x0000000100eb004c ffi_call_SYSV + 76\n  [bt] (5) 6   libffi.8.dylib                      0x0000000100ead7d4 ffi_call_int + 1336\n  [bt] (6) 7   _ctypes.cpython-310-darwin.so       0x0000000100e9011c _ctypes_callproc + 944\n  [bt] (7) 8   _ctypes.cpython-310-darwin.so       0x0000000100e8a3f8 PyCFuncPtr_call + 228\n  [bt] (8) 9   python3.10                          0x00000001006ef000 _PyEval_EvalFrameDefault + 58988\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the hyperparameters for the XGBoost classifier\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'missing': 1,\n",
    "    'gamma': 0,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'reg_lambda': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Define the best hyperparameters\n",
    "best_hyperparameters = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0\n",
    "}\n",
    "\n",
    "# Update the xgb_params dictionary with the best hyperparameters\n",
    "xgb_params.update(best_hyperparameters)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Initialize and train the XGBoost classifier with the best hyperparameters\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_clf.fit(X_train, y_train, verbose=0, eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "# Prepare evaluation metric plots\n",
    "results = xgb_clf.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# Plot mlogloss\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('mlogloss')\n",
    "plt.title('XGBoost mlogloss')\n",
    "plt.show()\n",
    "\n",
    "# Plot merror\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.plot(x_axis, results['validation_0']['merror'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['merror'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('merror')\n",
    "plt.title('XGBoost merror')\n",
    "plt.show()\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Print confusion matrix and evaluation metrics\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('---------------------- XGBoost ----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 1.0\n",
      "Confidence Interval: [1.0, 1.0]\n",
      "Average Precision: 1.0\n",
      "Average Recall: 1.0\n",
      "Average F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters for the XGBoost classifier\n",
    "xgb_params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'missing': 1,\n",
    "    'gamma': 0,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'reg_lambda': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_metric': ['merror', 'mlogloss'],\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Define the best hyperparameters\n",
    "best_hyperparameters = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0\n",
    "}\n",
    "\n",
    "# Update the xgb_params dictionary with the best hyperparameters\n",
    "xgb_params.update(best_hyperparameters)\n",
    "\n",
    "# Initialize lists to store results for each repetition\n",
    "accuracies = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "# Repeat the process 100 times\n",
    "for _ in range(100):\n",
    "    # Initialize lists to store results for each fold\n",
    "    fold_actual_labels = []\n",
    "    fold_predicted_labels = []\n",
    "\n",
    "    # Perform nested Leave-10-Out cross-validation\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    for train_index, test_index in kfold.split(df):\n",
    "        x_train, x_test = df.iloc[train_index], df.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Feature selection using Elastic Net\n",
    "        elastic_net = ElasticNet(l1_ratio=0.5, alpha=0.5, max_iter=10000)\n",
    "        elastic_net.fit(x_train, y_train)\n",
    "      # Select features based on Elastic Net feature importances\n",
    "        sfm = SelectFromModel(elastic_net)\n",
    "        sfm.fit(x_train, y_train)\n",
    "    \n",
    "        # Transform the datasets to include only selected features\n",
    "        x_train_selected = sfm.transform(x_train)\n",
    "        x_test_selected = sfm.transform(x_test)\n",
    "\n",
    "        # Update the xgb_params dictionary with the best hyperparameters\n",
    "        xgb_params.update(best_hyperparameters)\n",
    "\n",
    "        # Initialize and train the XGBoost classifier with the best hyperparameters\n",
    "        xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "        xgb_clf.fit(x_train_selected, y_train, verbose=0, eval_set=[(x_train_selected, y_train), (x_test_selected, y_test)])\n",
    "\n",
    "        # Get predictions on the test set\n",
    "        y_pred = xgb_clf.predict(x_test_selected)\n",
    "\n",
    "        # Append the actual and predicted labels to the lists for this fold\n",
    "        fold_actual_labels.extend(y_test)\n",
    "        fold_predicted_labels.extend(y_pred)\n",
    "\n",
    "    # Calculate performance metrics for this repetition\n",
    "    accuracy = accuracy_score(fold_actual_labels, fold_predicted_labels)\n",
    "    classification_report_str = classification_report(fold_actual_labels, fold_predicted_labels, output_dict=True)\n",
    "\n",
    "    # Store results for this repetition\n",
    "    accuracies.append(accuracy)\n",
    "    precision_list.append(classification_report_str['macro avg']['precision'])\n",
    "    recall_list.append(classification_report_str['macro avg']['recall'])\n",
    "    f1_score_list.append(classification_report_str['macro avg']['f1-score'])\n",
    "\n",
    "# Calculate the average accuracy and classification report over 100 repetitions\n",
    "average_accuracy = np.mean(accuracies)\n",
    "average_precision = np.mean(precision_list)\n",
    "average_recall = np.mean(recall_list)\n",
    "average_f1_score = np.mean(f1_score_list)\n",
    "\n",
    "# Compute 95% confidence intervals using the percentile method\n",
    "confidence_interval_lower = np.percentile(accuracies, 2.5)\n",
    "confidence_interval_upper = np.percentile(accuracies, 97.5)\n",
    "\n",
    "# Print the summary results\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n",
    "print(f\"Confidence Interval: [{confidence_interval_lower}, {confidence_interval_upper}]\")\n",
    "print(f\"Average Precision: {average_precision}\")\n",
    "print(f\"Average Recall: {average_recall}\")\n",
    "print(f\"Average F1-Score: {average_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.x (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.15.0rc0, 2.15.0rc1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.x\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: keras==2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: scikeras in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikeras) (1.2.2)\n",
      "Requirement already satisfied: tensorflow-metal<2.0.0,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikeras) (1.1.0)\n",
      "Requirement already satisfied: packaging>=0.21 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikeras) (23.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from scikit-learn>=1.0.0->scikeras) (1.23.1)\n",
      "Requirement already satisfied: six>=1.15.0 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from tensorflow-metal<2.0.0,>=1.1.0->scikeras) (1.16.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/homebrew/Caskroom/miniforge/base/envs/lupus/lib/python3.10/site-packages (from tensorflow-metal<2.0.0,>=1.1.0->scikeras) (0.38.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.x\n",
    "!pip install keras==2.7\n",
    "!pip install scikeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with the actual file)\n",
    "file_path = \"/Users/Sebastiano/data/ML_MRI copy.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "data = data.drop(['Patient', 'Gender', 'Age','Event', 'Scale factor', 'SNR'], axis = 'columns')\n",
    "\n",
    "# Assuming the 'Class' column contains the labels\n",
    "X = data.drop('NP-SLE', axis=1)\n",
    "y = data['NP-SLE']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "decoded = Dense(X_train.shape[1], activation='linear')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder (you can adjust the number of epochs)\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Extract features using the encoder part of the autoencoder\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Sebastiano/SLE_ML/XGBoost.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m xgb_classifier \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m'\u001b[39m, num_class\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m xgb_grid \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mxgb_classifier, param_grid\u001b[39m=\u001b[39mxgb_params, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m xgb_grid\u001b[39m.\u001b[39mfit(X_train_encoded, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Get the best XGBoost classifier\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Sebastiano/SLE_ML/XGBoost.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m best_xgb_classifier \u001b[39m=\u001b[39m xgb_grid\u001b[39m.\u001b[39mbest_estimator_\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_encoded' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for XGBoost hyperparameter tuning\n",
    "xgb_params = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "}\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=3, seed=42)\n",
    "xgb_grid = GridSearchCV(estimator=xgb_classifier, param_grid=xgb_params, cv=3)\n",
    "xgb_grid.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Get the best XGBoost classifier\n",
    "best_xgb_classifier = xgb_grid.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_xgb_classifier.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the best XGBoost model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "#print(\"Best Autoencoder Parameters:\", autoencoder_grid.best_params_)\n",
    "#print(\"Best XGBoost Parameters:\", xgb_grid.best_params_)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
